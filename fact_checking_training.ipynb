{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-10T19:46:49.239542Z",
     "start_time": "2025-01-10T19:46:45.986867Z"
    }
   },
   "source": [
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "from rdflib import Graph, URIRef, RDF\n",
    "from pykeen.triples import TriplesFactory\n",
    "from pykeen.pipeline import pipeline\n"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the knowledge graph"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-10T19:47:04.666475Z",
     "start_time": "2025-01-10T19:46:49.250549Z"
    }
   },
   "source": [
    "reference_knowledge_graph = Graph()\n",
    "reference_knowledge_graph.parse(\"data/reference-kg.nt\")"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Graph identifier=N37520aeaf52a4808bd31672a6acdc2f1 (<class 'rdflib.graph.Graph'>)>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting the graph to PyKeen triples"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-10T19:47:05.091984Z",
     "start_time": "2025-01-10T19:47:04.865173Z"
    }
   },
   "source": [
    "# reference_data_numpy = np.array(list(reference_knowledge_graph), dtype=str)\n",
    "# reference_data_pykeen = TriplesFactory.from_labeled_triples(reference_data_numpy)\n",
    "# reference_data_pykeen.create_inverse_triples = True\n",
    "\n",
    "### to create a new model, execute the lines above instead of this one (requires a lot of RAM)\n",
    "reference_data_pykeen = TriplesFactory.from_path_binary(\"trans-e-embeddings/training_triples\")"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\janne\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pykeen\\triples\\triples_factory.py:740: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  data = dict(torch.load(path.joinpath(cls.base_file_name)))\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning the entity/relation embeddings"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-10T19:47:05.113097Z",
     "start_time": "2025-01-10T19:47:05.098706Z"
    }
   },
   "source": [
    "# training, testing, validation = reference_data_pykeen.split([0.8, 0.1, 0.1])\n",
    "\n",
    "# result = pipeline(\n",
    "#     training=training,\n",
    "#     testing=testing,\n",
    "#     validation=validation,\n",
    "#     model='TransE',\n",
    "#     model_kwargs={\n",
    "#         'embedding_dim': 50\n",
    "#     },\n",
    "#     epochs=25\n",
    "# )\n",
    "\n",
    "# result.save_to_directory(\"trans-e-embeddings\")\n",
    "# model = result.model\n",
    "\n",
    "### to create a new model, execute the lines above instead of this one\n",
    "model = torch.load(\"trans-e-embeddings/trained_model.pkl\", map_location=torch.device('cpu'))"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\janne\\AppData\\Local\\Temp\\ipykernel_18628\\3822165361.py:18: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model = torch.load(\"trans-e-embeddings/trained_model.pkl\", map_location=torch.device('cpu'))\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the training/testing data"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-10T18:53:19.597610Z",
     "start_time": "2025-01-10T18:10:30.295240Z"
    }
   },
   "source": [
    "training_graph = Graph()\n",
    "training_graph.parse(\"data/fokg-sw-train-2024.nt\")\n",
    "\n",
    "training_triples = []\n",
    "training_labels = []\n",
    "\n",
    "for statement in training_graph.subjects(RDF.type, RDF.Statement):\n",
    "    \n",
    "    subject = training_graph.value(statement, RDF.subject)\n",
    "    predicate = training_graph.value(statement, RDF.predicate)\n",
    "    obj = training_graph.value(statement, RDF.object)\n",
    "\n",
    "    subject_id = reference_data_pykeen.entity_to_id[subject.n3().strip(\"<>\")]\n",
    "    predicate_id = reference_data_pykeen.relation_to_id[predicate.n3().strip(\"<>\")]\n",
    "    obj_id = reference_data_pykeen.entity_to_id[obj.n3().strip(\"<>\")]\n",
    "\n",
    "    subject_tensor = model.entity_representations[0](torch.LongTensor([subject_id]))\n",
    "    predicate_tensor = model.relation_representations[0](torch.LongTensor([predicate_id]))\n",
    "    obj_tensor = model.entity_representations[0](torch.LongTensor([obj_id]))\n",
    "\n",
    "    veracity_score = training_graph.value(statement, URIRef(\"http://swc2017.aksw.org/hasTruthValue\"))\n",
    "    \n",
    "    training_triples.append(torch.cat((subject_tensor, predicate_tensor, obj_tensor), dim=1))\n",
    "    training_labels.append(float(veracity_score))\n",
    "\n",
    "# generate 20000 additional training triples (10k true triples, 10k false triples)\n",
    "for s, p, o in random.choices(list(reference_knowledge_graph), k=10000):\n",
    "\n",
    "    subject_id = reference_data_pykeen.entity_to_id[str(s)]\n",
    "    predicate_id = reference_data_pykeen.relation_to_id[str(p)]\n",
    "    obj_id = reference_data_pykeen.entity_to_id[str(o)]\n",
    "\n",
    "    subject_tensor = model.entity_representations[0](torch.LongTensor([subject_id]))\n",
    "    predicate_tensor = model.relation_representations[0](torch.LongTensor([predicate_id]))\n",
    "    obj_tensor = model.entity_representations[0](torch.LongTensor([obj_id]))\n",
    "\n",
    "    training_triples.append(torch.cat((subject_tensor, predicate_tensor, obj_tensor), dim=1))\n",
    "    training_labels.append(1.0)\n",
    "\n",
    "    while True:\n",
    "\n",
    "        replacement_choice = random.choice([\"subject\", \"predicate\", \"object\"])\n",
    "\n",
    "        if replacement_choice == \"subject\":\n",
    "            s_prime = random.choice(list(reference_knowledge_graph.subjects(unique=True)))\n",
    "            if (s_prime, p, o) not in reference_knowledge_graph:\n",
    "                s_prime_id = reference_data_pykeen.entity_to_id[str(s_prime)]\n",
    "                s_prime_tensor = model.entity_representations[0](torch.LongTensor([s_prime_id]))\n",
    "                training_triples.append(torch.cat((s_prime_tensor, predicate_tensor, obj_tensor), dim=1))\n",
    "                break\n",
    "        elif replacement_choice == \"predicate\":\n",
    "            p_prime = random.choice(list(reference_knowledge_graph.predicates(unique=True)))\n",
    "            if (s, p_prime, o) not in reference_knowledge_graph:\n",
    "                p_prime_id = reference_data_pykeen.relation_to_id[str(p_prime)]\n",
    "                p_prime_tensor = model.relation_representations[0](torch.LongTensor([p_prime_id]))\n",
    "                training_triples.append(torch.cat((subject_tensor, p_prime_tensor, obj_tensor), dim=1))\n",
    "                break\n",
    "        elif replacement_choice == \"object\":\n",
    "            o_prime = random.choice(list(reference_knowledge_graph.objects(unique=True)))\n",
    "            if (s, p, o_prime) not in reference_knowledge_graph:\n",
    "                o_prime_id = reference_data_pykeen.entity_to_id[str(o_prime)]\n",
    "                o_prime_tensor = model.entity_representations[0](torch.LongTensor([o_prime_id]))\n",
    "                training_triples.append(torch.cat((subject_tensor, predicate_tensor, o_prime_tensor), dim=1))\n",
    "                break\n",
    "\n",
    "    training_labels.append(0.0)\n",
    "\n",
    "split_index = int(len(training_triples) * 0.8)\n",
    "\n",
    "X_train = torch.stack(training_triples[:split_index])\n",
    "y_train = torch.Tensor(training_labels[:split_index])\n",
    "\n",
    "X_test = torch.stack(training_triples[split_index:])\n",
    "y_test = torch.Tensor(training_labels[split_index:])\n"
   ],
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[12], line 59\u001B[0m\n\u001B[0;32m     57\u001B[0m         \u001B[38;5;28;01mbreak\u001B[39;00m\n\u001B[0;32m     58\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m replacement_choice \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mobject\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m---> 59\u001B[0m     o_prime \u001B[38;5;241m=\u001B[39m random\u001B[38;5;241m.\u001B[39mchoice(\u001B[38;5;28mlist\u001B[39m(reference_knowledge_graph\u001B[38;5;241m.\u001B[39mobjects(unique\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)))\n\u001B[0;32m     60\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m (s, p, o_prime) \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m reference_knowledge_graph:\n\u001B[0;32m     61\u001B[0m         o_prime_id \u001B[38;5;241m=\u001B[39m reference_data_pykeen\u001B[38;5;241m.\u001B[39mentity_to_id[\u001B[38;5;28mstr\u001B[39m(o_prime)]\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\rdflib\\graph.py:869\u001B[0m, in \u001B[0;36mGraph.objects\u001B[1;34m(self, subject, predicate, unique)\u001B[0m\n\u001B[0;32m    867\u001B[0m objs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n\u001B[0;32m    868\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m s, p, o \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtriples((subject, predicate, \u001B[38;5;28;01mNone\u001B[39;00m)):\n\u001B[1;32m--> 869\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[43mo\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mnot\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mobjs\u001B[49m:\n\u001B[0;32m    870\u001B[0m         \u001B[38;5;28;01myield\u001B[39;00m o\n\u001B[0;32m    871\u001B[0m         \u001B[38;5;28;01mtry\u001B[39;00m:\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\rdflib\\term.py:174\u001B[0m, in \u001B[0;36mIdentifier.__eq__\u001B[1;34m(self, other)\u001B[0m\n\u001B[0;32m    171\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__ne__\u001B[39m(\u001B[38;5;28mself\u001B[39m, other: Any) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28mbool\u001B[39m:\n\u001B[0;32m    172\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__eq__\u001B[39m(other)\n\u001B[1;32m--> 174\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__eq__\u001B[39m(\u001B[38;5;28mself\u001B[39m, other: Any) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28mbool\u001B[39m:\n\u001B[0;32m    175\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    176\u001B[0m \u001B[38;5;124;03m    Equality for Nodes.\u001B[39;00m\n\u001B[0;32m    177\u001B[0m \n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    191\u001B[0m \u001B[38;5;124;03m    False\u001B[39;00m\n\u001B[0;32m    192\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m    194\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mtype\u001B[39m(\u001B[38;5;28mself\u001B[39m) \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28mtype\u001B[39m(other):\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the classifier"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-10T18:00:30.013435Z",
     "start_time": "2025-01-10T18:00:28.612190Z"
    }
   },
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "classifier = torch.nn.Sequential(\n",
    "    torch.nn.Linear(in_features=150, out_features=300),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(in_features=300, out_features=100),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(in_features=100, out_features=1),\n",
    ").to(device)\n",
    "\n",
    "loss_function = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "def accuracy_function(y_true, y_pred):\n",
    "    correct = torch.eq(y_true, y_pred).sum().item()\n",
    "    acc = (correct / len(y_pred)) * 100 \n",
    "    return acc\n",
    "\n",
    "optimizer = torch.optim.Adam(params=classifier.parameters(), lr=0.01, weight_decay=0.001)"
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the classifier"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-10T18:00:41.022135Z",
     "start_time": "2025-01-10T18:00:40.960314Z"
    }
   },
   "source": [
    "X_train, y_train = X_train.to(device), y_train.to(device)\n",
    "X_test, y_test = X_test.to(device), y_test.to(device)\n",
    "\n",
    "epochs = 50\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # set model to training mode\n",
    "    classifier.train()\n",
    "\n",
    "    # forward pass\n",
    "    y_logits = classifier(X_train).squeeze() # squeeze to remove extra dimensions\n",
    "    y_pred = torch.round(torch.sigmoid(y_logits)) # logits -> prediction probabilities -> prediction labels\n",
    "  \n",
    "    # calculate loss and accuracy\n",
    "    loss = loss_function(y_logits, y_train)\n",
    "    acc = accuracy_function(y_true=y_train, y_pred=y_pred)\n",
    "\n",
    "    # set gradients to zero\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # backpropagation\n",
    "    loss.backward(retain_graph=True)\n",
    "\n",
    "    # update weights\n",
    "    optimizer.step()\n",
    "\n",
    "    # set model to testing mode\n",
    "    classifier.eval()\n",
    "    with torch.inference_mode():\n",
    "\n",
    "        # predict test labels\n",
    "        test_logits = classifier(X_test).squeeze() \n",
    "        test_pred = torch.round(torch.sigmoid(test_logits))\n",
    "\n",
    "        # calculate loss and accuracy\n",
    "        test_loss = loss_function(test_logits, y_test)\n",
    "        test_acc = accuracy_function(y_true=y_test, y_pred=test_pred)\n",
    "\n",
    "    # print stats\n",
    "    print(f\"Epoch: {epoch} | Training Loss: {loss:.5f}, Training Accuracy: {acc:.2f}% | Test Loss: {test_loss:.5f}, Test Accuracy: {test_acc:.2f}%\")\n"
   ],
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[11], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m X_train, y_train \u001B[38;5;241m=\u001B[39m \u001B[43mX_train\u001B[49m\u001B[38;5;241m.\u001B[39mto(device), y_train\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[0;32m      2\u001B[0m X_test, y_test \u001B[38;5;241m=\u001B[39m X_test\u001B[38;5;241m.\u001B[39mto(device), y_test\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[0;32m      4\u001B[0m epochs \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m50\u001B[39m\n",
      "\u001B[1;31mNameError\u001B[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Predicting test labels example"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-10T19:58:18.465240Z",
     "start_time": "2025-01-10T19:58:18.417523Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# set device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# define classifier architecture\n",
    "classifier = torch.nn.Sequential(\n",
    "    torch.nn.Linear(in_features=150, out_features=300),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(in_features=300, out_features=100),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(in_features=100, out_features=1),\n",
    ").to(device)\n",
    "\n",
    "# load the saved model parameters\n",
    "classifier.load_state_dict(torch.load('classifier/fact_checking_model.pt', map_location=torch.device('cpu')))\n",
    "\n",
    "# load test graph\n",
    "test_graph = Graph()\n",
    "test_graph.parse(\"data/fokg-sw-test-2024.nt\")"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\janne\\AppData\\Local\\Temp\\ipykernel_18628\\2360505145.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  classifier.load_state_dict(torch.load('fact_checking_model.pt', map_location=torch.device('cpu')))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Graph identifier=N2238622ff8644a7daca6d2406858fe96 (<class 'rdflib.graph.Graph'>)>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-10T19:58:27.064704Z",
     "start_time": "2025-01-10T19:58:26.920059Z"
    }
   },
   "cell_type": "code",
   "source": [
    "test_triple_iris = []\n",
    "test_triples = []\n",
    "test_labels = []\n",
    "\n",
    "# process test graph similar to training data\n",
    "for statement in test_graph.subjects(RDF.type, RDF.Statement):\n",
    "\n",
    "    subject = test_graph.value(statement, RDF.subject)\n",
    "    predicate = test_graph.value(statement, RDF.predicate)\n",
    "    obj = test_graph.value(statement, RDF.object)\n",
    "\n",
    "    subject_id = reference_data_pykeen.entity_to_id[str(subject)]\n",
    "    predicate_id = reference_data_pykeen.relation_to_id[str(predicate)]\n",
    "    obj_id = reference_data_pykeen.entity_to_id[str(obj)]\n",
    "\n",
    "    subject_tensor = model.entity_representations[0](torch.LongTensor([subject_id]))\n",
    "    predicate_tensor = model.relation_representations[0](torch.LongTensor([predicate_id]))\n",
    "    obj_tensor = model.entity_representations[0](torch.LongTensor([obj_id]))\n",
    "\n",
    "    test_triple_iris.append(statement)\n",
    "    test_triples.append(torch.cat((subject_tensor, predicate_tensor, obj_tensor), dim=1))\n",
    "\n",
    "X_test = torch.stack(test_triples)"
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-10T19:58:27.859293Z",
     "start_time": "2025-01-10T19:58:27.840929Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# set classifier to evaluation\n",
    "classifier.eval()\n",
    "\n",
    "test_logits = classifier(X_test).squeeze()\n",
    "#test_pred = torch.round(torch.sigmoid(test_logits))\n",
    "test_pred = torch.sigmoid(test_logits)"
   ],
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-10T19:58:29.032051Z",
     "start_time": "2025-01-10T19:58:29.019545Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# save predictions as list\n",
    "test_pred_as_list = [x.item() for x in list(test_pred)]\n",
    "\n",
    "# write predictions to file\n",
    "result_file = open('result.ttl', 'w')\n",
    "for i in range(len(test_pred_as_list)):\n",
    "    print(f\"<{test_triple_iris[i]}> <http://swc2017.aksw.org/hasTruthValue> \\\"{test_pred_as_list[i]}\\\"^^<http://www.w3.org/2001/XMLSchema#double> .\", file=result_file)\n",
    "result_file.close()"
   ],
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
