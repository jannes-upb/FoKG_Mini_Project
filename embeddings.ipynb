{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import rdflib\n",
    "from rdflib import URIRef\n",
    "from pykeen.triples import TriplesFactory\n",
    "from pykeen.pipeline import pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading knowledge graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_knowledge_graph = rdflib.Graph()\n",
    "reference_knowledge_graph.parse(\"data/reference-kg.nt\")\n",
    "\n",
    "reference_knowledge_graph_trimmed = rdflib.Graph()\n",
    "reference_knowledge_graph_trimmed.parse(\"data/reference-kg-1000.nt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting to pykeen format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_data_numpy = np.array([(s, p, o) for s, p, o in reference_knowledge_graph])\n",
    "reference_data_pykeen = TriplesFactory.from_labeled_triples(reference_data_numpy)\n",
    "reference_data_pykeen.create_inverse_triples = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training, testing, validation = reference_data_pykeen.split([0.8, 0.1, 0.1])\n",
    "\n",
    "result = pipeline(\n",
    "    training=training,\n",
    "    testing=testing,\n",
    "    validation=validation,\n",
    "    model='TransE',\n",
    "    model_kwargs={\n",
    "        'embedding_dim': 150,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the training/testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = rdflib.Graph()\n",
    "train.parse(\"data/fokg-sw-train-2024.nt\")\n",
    "\n",
    "test = rdflib.Graph()\n",
    "test.parse(\"data/fokg-sw-test-2024.nt\")\n",
    "\n",
    "train_data = []\n",
    "train_labels = []\n",
    "\n",
    "for statement in train.subjects(URIRef(\"http://www.w3.org/1999/02/22-rdf-syntax-ns#type\"), URIRef(\"http://www.w3.org/1999/02/22-rdf-syntax-ns#Statement\")):\n",
    "    subject = train.value(statement, URIRef(\"http://www.w3.org/1999/02/22-rdf-syntax-ns#subject\"))\n",
    "    predicate = train.value(statement, URIRef(\"http://www.w3.org/1999/02/22-rdf-syntax-ns#predicate\"))\n",
    "    obj = train.value(statement, URIRef(\"http://www.w3.org/1999/02/22-rdf-syntax-ns#object\"))\n",
    "\n",
    "    subject_id = reference_data_pykeen.entity_to_id[subject.n3().strip(\"<>\")]\n",
    "    predicate_id = reference_data_pykeen.entity_to_id[predicate.n3().strip(\"<>\")]\n",
    "    obj_id = reference_data_pykeen.entity_to_id[obj.n3().strip(\"<>\")]\n",
    "\n",
    "    subject_tensor = result.model.entity_representations[0](torch.LongTensor([subject_id]))\n",
    "    predicate_tensor = result.model.relation_representations[0](torch.LongTensor([predicate_id]))\n",
    "    obj_tensor = result.model.entity_representations[0](torch.LongTensor([obj_id]))\n",
    "\n",
    "    veracity_score = train.value(statement, URIRef(\"http://swc2017.aksw.org/hasTruthValue\"))\n",
    "    \n",
    "    if subject and predicate and obj and veracity_score:\n",
    "        # train_data.append((subject_id, predicate_id, obj_id))\n",
    "        train_data.append(torch.cat(torch.LongTensor(subject_tensor), torch.LongTensor(predicate_tensor), torch.LongTensor(obj_tensor)))\n",
    "        train_labels.append(float(veracity_score))\n",
    "\n",
    "split_index = int(len(train_data) * 0.8)\n",
    "\n",
    "X_train = train_data[:split_index]\n",
    "y_train = train_labels[:split_index]\n",
    "\n",
    "X_test = train_data[split_index:]\n",
    "y_test = train_labels[split_index:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = nn.Sequential(\n",
    "    nn.Linear(in_features=150, out_features=300),\n",
    "    nn.Linear(in_features=300, out_features=100),\n",
    "    nn.Linear(in_features=100, out_features=1)\n",
    ")\n",
    "\n",
    "loss_function = nn.BCEWithLogitsLoss()\n",
    "\n",
    "def accuracy_fn(y_true, y_pred):\n",
    "    correct = torch.eq(y_true, y_pred).sum().item()\n",
    "    acc = (correct / len(y_pred)) * 100 \n",
    "    return acc\n",
    "\n",
    "optimizer = torch.optim.Adam(params=classifier.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "epochs = 100\n",
    "\n",
    "# Put data to target device\n",
    "train_data, train_labels = train_data.to(device), train_labels.to(device)\n",
    "X_test, y_test = X_test.to(device), y_test.to(device)\n",
    "\n",
    "# Build training and evaluation loop\n",
    "for epoch in range(epochs):\n",
    "    ### Training\n",
    "    classifier.train()\n",
    "    \n",
    "    # 1. Forward pass (model outputs raw logits)\n",
    "    y_logits = classifier(train_data).squeeze() # squeeze to remove extra `1` dimensions, this won't work unless model and data are on same device \n",
    "    y_pred = torch.round(torch.sigmoid(y_logits)) # turn logits -> pred probs -> pred labls\n",
    "  \n",
    "    # 2. Calculate loss/accuracy\n",
    "    # loss = loss_fn(torch.sigmoid(y_logits), # Using nn.BCELoss you need torch.sigmoid()\n",
    "    #                y_train) \n",
    "    loss = loss_function(y_logits, # Using nn.BCEWithLogitsLoss works with raw logits\n",
    "                   train_labels) \n",
    "    acc = accuracy_fn(y_true=train_labels, \n",
    "                      y_pred=y_pred) \n",
    "\n",
    "    # 3. Optimizer zero grad\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # 4. Loss backwards\n",
    "    loss.backward()\n",
    "\n",
    "    # 5. Optimizer step\n",
    "    optimizer.step()\n",
    "\n",
    "    ### Testing\n",
    "    classifier.eval()\n",
    "    with torch.inference_mode():\n",
    "        # 1. Forward pass\n",
    "        test_logits = classifier(X_test).squeeze() \n",
    "        test_pred = torch.round(torch.sigmoid(test_logits))\n",
    "        # 2. Caculate loss/accuracy\n",
    "        test_loss = classifier(test_logits,\n",
    "                            y_test)\n",
    "        test_acc = accuracy_fn(y_true=y_test,\n",
    "                               y_pred=test_pred)\n",
    "\n",
    "    # Print out what's happening every 10 epochs\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch: {epoch} | Loss: {loss:.5f}, Accuracy: {acc:.2f}% | Test loss: {test_loss:.5f}, Test acc: {test_acc:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
