{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "from rdflib import Graph, URIRef, RDF\n",
    "from pykeen.triples import TriplesFactory\n",
    "from pykeen.pipeline import pipelinej\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the knowledge graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_knowledge_graph = Graph()\n",
    "reference_knowledge_graph.parse(\"data/reference-kg.nt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting the graph to PyKeen triples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reference_data_numpy = np.array(list(reference_knowledge_graph), dtype=str)\n",
    "# reference_data_pykeen = TriplesFactory.from_labeled_triples(reference_data_numpy)\n",
    "# reference_data_pykeen.create_inverse_triples = True\n",
    "\n",
    "### to create a new model, execute the lines above instead of this one (requires a lot of RAM)\n",
    "reference_data_pykeen = TriplesFactory.from_path_binary(\"trans-e-embeddings/training_triples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning the entity/relation embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training, testing, validation = reference_data_pykeen.split([0.8, 0.1, 0.1])\n",
    "\n",
    "# result = pipeline(\n",
    "#     training=training,\n",
    "#     testing=testing,\n",
    "#     validation=validation,\n",
    "#     model='TransE',\n",
    "#     model_kwargs={\n",
    "#         'embedding_dim': 50\n",
    "#     },\n",
    "#     epochs=25\n",
    "# )\n",
    "\n",
    "# result.save_to_directory(\"trans-e-embeddings\")\n",
    "# model = result.model\n",
    "\n",
    "### to create a new model, execute the lines above instead of this one\n",
    "model = torch.load(\"trans-e-embeddings/trained_model.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the training/testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_graph = Graph()\n",
    "training_graph.parse(\"data/fokg-sw-train-2024.nt\")\n",
    "\n",
    "training_triples = []\n",
    "training_labels = []\n",
    "\n",
    "for statement in training_graph.subjects(RDF.type, RDF.Statement):\n",
    "    \n",
    "    subject = training_graph.value(statement, RDF.subject)\n",
    "    predicate = training_graph.value(statement, RDF.predicate)\n",
    "    obj = training_graph.value(statement, RDF.object)\n",
    "\n",
    "    subject_id = reference_data_pykeen.entity_to_id[subject.n3().strip(\"<>\")]\n",
    "    predicate_id = reference_data_pykeen.relation_to_id[predicate.n3().strip(\"<>\")]\n",
    "    obj_id = reference_data_pykeen.entity_to_id[obj.n3().strip(\"<>\")]\n",
    "\n",
    "    subject_tensor = model.entity_representations[0](torch.LongTensor([subject_id]))\n",
    "    predicate_tensor = model.relation_representations[0](torch.LongTensor([predicate_id]))\n",
    "    obj_tensor = model.entity_representations[0](torch.LongTensor([obj_id]))\n",
    "\n",
    "    veracity_score = training_graph.value(statement, URIRef(\"http://swc2017.aksw.org/hasTruthValue\"))\n",
    "    \n",
    "    training_triples.append(torch.cat((subject_tensor, predicate_tensor, obj_tensor), dim=1))\n",
    "    training_labels.append(float(veracity_score))\n",
    "\n",
    "# generate 20000 additional training triples (10k true triples, 10k false triples)\n",
    "for s, p, o in random.choices(list(reference_knowledge_graph), k=10000):\n",
    "\n",
    "    subject_id = reference_data_pykeen.entity_to_id[str(s)]\n",
    "    predicate_id = reference_data_pykeen.relation_to_id[str(p)]\n",
    "    obj_id = reference_data_pykeen.entity_to_id[str(o)]\n",
    "\n",
    "    subject_tensor = model.entity_representations[0](torch.LongTensor([subject_id]))\n",
    "    predicate_tensor = model.relation_representations[0](torch.LongTensor([predicate_id]))\n",
    "    obj_tensor = model.entity_representations[0](torch.LongTensor([obj_id]))\n",
    "\n",
    "    training_triples.append(torch.cat((subject_tensor, predicate_tensor, obj_tensor), dim=1))\n",
    "    training_labels.append(1.0)\n",
    "\n",
    "    while True:\n",
    "\n",
    "        replacement_choice = random.choice([\"subject\", \"predicate\", \"object\"])\n",
    "\n",
    "        if replacement_choice == \"subject\":\n",
    "            s_prime = random.choice(list(reference_knowledge_graph.subjects(unique=True)))\n",
    "            if (s_prime, p, o) not in reference_knowledge_graph:\n",
    "                s_prime_id = reference_data_pykeen.entity_to_id[str(s_prime)]\n",
    "                s_prime_tensor = model.entity_representations[0](torch.LongTensor([s_prime_id]))\n",
    "                training_triples.append(torch.cat((s_prime_tensor, predicate_tensor, obj_tensor), dim=1))\n",
    "                break\n",
    "        elif replacement_choice == \"predicate\":\n",
    "            p_prime = random.choice(list(reference_knowledge_graph.predicates(unique=True)))\n",
    "            if (s, p_prime, o) not in reference_knowledge_graph:\n",
    "                p_prime_id = reference_data_pykeen.relation_to_id[str(p_prime)]\n",
    "                p_prime_tensor = model.relation_representations[0](torch.LongTensor([p_prime_id]))\n",
    "                training_triples.append(torch.cat((subject_tensor, p_prime_tensor, obj_tensor), dim=1))\n",
    "                break\n",
    "        elif replacement_choice == \"object\":\n",
    "            o_prime = random.choice(list(reference_knowledge_graph.objects(unique=True)))\n",
    "            if (s, p, o_prime) not in reference_knowledge_graph:\n",
    "                o_prime_id = reference_data_pykeen.entity_to_id[str(o_prime)]\n",
    "                o_prime_tensor = model.entity_representations[0](torch.LongTensor([o_prime_id]))\n",
    "                training_triples.append(torch.cat((subject_tensor, predicate_tensor, o_prime_tensor), dim=1))\n",
    "                break\n",
    "\n",
    "    training_labels.append(0.0)\n",
    "\n",
    "split_index = int(len(training_triples) * 0.8)\n",
    "\n",
    "X_train = torch.stack(training_triples[:split_index])\n",
    "y_train = torch.Tensor(training_labels[:split_index])\n",
    "\n",
    "X_test = torch.stack(training_triples[split_index:])\n",
    "y_test = torch.Tensor(training_labels[split_index:])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "classifier = torch.nn.Sequential(\n",
    "    torch.nn.Linear(in_features=150, out_features=300),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(in_features=300, out_features=100),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(in_features=100, out_features=1),\n",
    ").to(device)\n",
    "\n",
    "loss_function = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "def accuracy_function(y_true, y_pred):\n",
    "    correct = torch.eq(y_true, y_pred).sum().item()\n",
    "    acc = (correct / len(y_pred)) * 100 \n",
    "    return acc\n",
    "\n",
    "optimizer = torch.optim.Adam(params=classifier.parameters(), lr=0.01, weight_decay=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = X_train.to(device), y_train.to(device)\n",
    "X_test, y_test = X_test.to(device), y_test.to(device)\n",
    "\n",
    "epochs = 50\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # set model to training mode\n",
    "    classifier.train()\n",
    "\n",
    "    # forward pass\n",
    "    y_logits = classifier(X_train).squeeze() # squeeze to remove extra dimensions\n",
    "    y_pred = torch.round(torch.sigmoid(y_logits)) # logits -> prediction probabilities -> prediction labels\n",
    "  \n",
    "    # calculate loss and accuracy\n",
    "    loss = loss_function(y_logits, y_train)\n",
    "    acc = accuracy_function(y_true=y_train, y_pred=y_pred)\n",
    "\n",
    "    # set gradients to zero\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # backpropagation\n",
    "    loss.backward(retain_graph=True)\n",
    "\n",
    "    # update weights\n",
    "    optimizer.step()\n",
    "\n",
    "    # set model to testing mode\n",
    "    classifier.eval()\n",
    "    with torch.inference_mode():\n",
    "\n",
    "        # predict test labels\n",
    "        test_logits = classifier(X_test).squeeze() \n",
    "        test_pred = torch.round(torch.sigmoid(test_logits))\n",
    "\n",
    "        # calculate loss and accuracy\n",
    "        test_loss = loss_function(test_logits, y_test)\n",
    "        test_acc = accuracy_function(y_true=y_test, y_pred=test_pred)\n",
    "\n",
    "    # print stats\n",
    "    print(f\"Epoch: {epoch} | Training Loss: {loss:.5f}, Training Accuracy: {acc:.2f}% | Test Loss: {test_loss:.5f}, Test Accuracy: {test_acc:.2f}%\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
